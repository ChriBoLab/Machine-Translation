{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Model  # This does not work!\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "\n",
    "\n",
    "http://www.statmt.org/europarl/\n",
    "\n",
    "Θα χρησιμοποιήσουμε το σύνολο δεδομένων Europarl που έχει ζεύγη προτάσεων στις περισσότερες ευρωπαϊκές γλώσσες. Τα δεδομένα δημιουργήθηκαν από την Ευρωπαϊκή Ένωση, η οποία μεταφράζει πολλές από τις επικοινωνίες τους στις γλώσσες των χωρών μελών της Ευρωπαϊκής Ένωσης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import europarl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποίησα το αγγλό-ελληνικό σύνολο δεδομένων που περιέχει περίπου 2 εκατομμύρια ζεύγη προτάσεων. Ο κωδικός για τα ελληνικά ειναι 'el'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code='el'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Προκειμένου ο decoder να γνωρίζει πότε πρέπει να ξεκινήσει και να τερματίσει μια πρόταση, πρέπει να σημειώσουμε την αρχή και το τέλος κάθε φράσης με λέξεις που πιθανότατα δεν εμφανίζονται στο σύνολο δεδομένων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function that will automatically download and extract the data-files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Download progress: 100.0%\n",
      "Download finished. Extracting files.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "europarl.maybe_download_and_extract(language_code=language_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Εισάγουμε τα δεδομένα με τις ελληνικές προτάσεις.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_src = europarl.load_data(english=False,\n",
    "                              language_code=language_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εισάγουμε τα δεδομένα με τις μεταφρασμένες αγγλικές προτάσεις.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dest = europarl.load_data(english=True,\n",
    "                               language_code=language_code,\n",
    "                               start=mark_start,\n",
    "                               end=mark_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Data\n",
    "\n",
    "Ένα τυχαίο παράδειγμα απο τα δεδομένα "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Όπως μπορέσατε να διαπιστώσετε, ο περίφημος \"ιός του έτους 2000\" δεν εμφανίσθηκε. Αντιθέτως, οι πολίτες ορισμένων χωρών μας υπήρξαν θύματα φυσικών καταστροφών, οι οποίες ήταν όντως φοβερές.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "Τα νευρικά δίκτυα δεν μπορούν να λειτουργήσουν άμεσα σε δεδομένα κειμένου. Χρησιμοποιούμε μια διαδικασία δύο σταδίων για να μετατρέψουμε κείμενο σε αριθμούς που μπορούν να χρησιμοποιηθούν σε ένα νευρικό δίκτυο. Το πρώτο βήμα είναι να μετατρέψoυμε τις λέξεις σε λεγόμενα integer-tokens. Το δεύτερο βήμα είναι η μετατροπή των tokens σε διανύσματα αριθμών κινητής υποδιαστολής (embeddings).\n",
    "Ορίζουμε τον μέγιστο αριθμό λέξεων στο λεξιλόγιό μας. Αυτό σημαίνει ότι θα χρησιμοποιήσουμε μόνο τις 5000 ***πιο συχνές λέξεις*** στο σύνολο δεδομένων. Χρησιμοποιούμε τον ίδιο αριθμό και για τις γλώσσες προέλευσης και προορισμού\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bοηθητική συνάρτηση \n",
    "\n",
    "Στόχος αυτής της συνάρτησης είναι να μετατρέπει τα δεδομένα της βάσης δεδομένων δηλαδή την κάθε πρόταση ξεχωριστά σε μία σειρά από tokens. Πλέον οι προτάσεις θα αναπαριστώνται από έναν πίνακα με integers. Το μοντέλο μας όμως χρειάζεται εισόδους με συγκεκριμένο μέγεθος. Δεν μπορεί η κάθε είσοδος να εχει  διαφορετικό μέγεθος. Για να το πετύχουμε αυτό θα καθαρίσουμε ένα σταθερό μέγεθος και οποιαδήποτε πρόταση είναι μικρότερη από αυτό το μέγεθος θα γεμιστή με κενό(0). \n",
    "Το μέγεθος καθορίζεται από την παρακάτω εντολή \n",
    "\n",
    "max_tokens = np.mean(self.num_tokens) \\ + 2 * np.std(self.num_tokens)\n",
    "                         \n",
    "\n",
    "Επίσης, αναστρέφουμε τις ακολουθίες των λέξεων, επειδή σύμφωνα με έρευνες  υποδηλώνεται  ότι αυτό μπορεί να βελτιώσει την απόδοση, ο λόγος που το κάνουμε αυτο ειναι οτι οι τελευταίες λέξεις που βλέπουμε από τον κωδικοποιητή ταιριάζουν με τις πρώτες λέξεις που παράγει ο  κωδικοποιητής.\n",
    "\n",
    "Τέλος η συνάρτηση έχει διάφορες λειτουργίες όπου μπορούμε και δίνουμε τον πίνακα με τα tokens και μας επιστρέφει την πρόταση η δίνουμε τη πρόταση και μας επιστρέφει τα tokens.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, padding,\n",
    "                 reverse=False, num_words=None):\n",
    "        \"\"\"\n",
    "        :param texts: List of strings. This is the data-set.\n",
    "        :param padding: Either 'post' or 'pre' padding.\n",
    "        :param reverse: Boolean whether to reverse token-lists.\n",
    "        :param num_words: Max number of words to use.\n",
    "        \"\"\"\n",
    "\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "\n",
    "        # Create the vocabulary from the texts.\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "\n",
    "        # Convert all texts to lists of integer-tokens.\n",
    "        #  sequences may have different lengths.\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the token-sequences.\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "        \n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        # The number of integer-tokens in each sequence.\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        # Max number of tokens to use in all sequences.\n",
    "        # We will pad / truncate all sequences to this length.\n",
    "        # This is a compromise so we save a lot of memory and\n",
    "        # only have to truncate maybe 5% of all the sequences.\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "\n",
    "        # Pad / truncate all token-sequences to the given length.\n",
    "        # This creates a 2-dim numpy matrix that is easier to use.\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens ):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "       \n",
    "        \n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "\n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        \"\"\"\n",
    "        Convert a single text-string to tokens with optional\n",
    "        reversal and padding.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to tokens. Note that we assume there is only\n",
    "        # a single text-string so we wrap it in a list.\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the tokens.\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "\n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            # Pad and truncate sequences to the given length.\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Υπάρχουν δύο τρόποι να γεμίσουμε το πίνακα με 0 είτε να αφήσουμε την πρόταση στο τέλος και να γεμίσουμε με μηδέν την αρχή του πίνακα είτε το αντίθετο. Είναι σημαντικό όμως να προσέξουμε ότι στην περίπτωση του encoder αν το μοντέλο δεχόταν πρώτα την πρόταση και μετά όλα τα υπόλοιπα μηδενικά πολύ πιθανό να άλλαζαν πάρα πολύ τα βάρη του, τι σημαίνει αυτό; ότι ολόκληρη πρόταση θα έχανε το νόημά της αν όμως το δίκτυο διάβαζε πρώτα τα μηδενικά τα βάρη του θα παρέμεναν στην ίδια ακριβώς κατάσταση γιατί τα μηδενικά δεν έχουν κανένα απολύτως νόημα για αυτόν και όταν λοιπόν θα ερχόταν η πρώτη λέξη τότε θα μετέβαλε τα βάρη του αναλόγως μέχρι να τελειώσει η πρόταση και τα βάρη του θα έμεναν στην κατάσταση που θα θέλαμε. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Τώρα δημιουργήστε ένα tokenizer για τη γλώσσα προέλευσης. Σημειώστε ότι πληκτρολογούμε μηδενικά στην αρχή ('pre') των ακολουθιών."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_src = TokenizerWrap(texts=data_src,\n",
    "                              padding='pre',\n",
    "                              reverse=True,\n",
    "                              num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Τώρα δημιουργήστε το tokenizer για τη γλώσσα προορισμού. Χρειάζεται ένα tokenizer για τις γλώσσες προέλευσης και προορισμού, επειδή τα λεξιλόγιά τους είναι διαφορετικά. Σημειώστε ότι αυτός ο tokenizer δεν αναστρέφει τις ακολουθίες και το padding ειναι στο τέλος ('post').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_dest = TokenizerWrap(texts=data_dest,\n",
    "                               padding='post',\n",
    "                               reverse=False,\n",
    "                               num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Παρατηρούμε ότι τα μήκη των sequences είναι διαφορετικά για τις γλώσσες προέλευσης και προορισμού. Αυτό συμβαίνει επειδή τα κείμενα με το ίδιο νόημα μπορεί να έχουν διαφορετικό αριθμό λέξεων στις δύο γλώσσες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1235976, 51)\n",
      "(1235976, 56)\n"
     ]
    }
   ],
   "source": [
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Παρακάτω βλέπουμε ποιός ακέραιος αντιστοιχεί για  την αρχή ενός κειμένου στη γλώσσα προορισμού."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω βλέπουμε ποιός ακέραιος αντιστοιχεί για  το τέλος ενός κειμένου στη γλώσσα προορισμού."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Token Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Αυτή είναι η έξοδος του tokenizer. Βλεπουμε πώς είναι γεμισμένο με μηδενικά στην αρχή (pre-padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0, 1118, 1449, 3385,    3,  292,   17,   36, 1412,\n",
       "       2936,   19,  787,    9,  121,  107,   66])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_src[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Μπορούμε να ανακατασκευάσουμε το αρχικό κείμενο μετατρέποντας κάθε token πίσω στην αντίστοιχη λέξη του:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'συνόδου περιόδου τρέχουσας της διάρκεια τη κατά ημέρες επόμενες τις θέματος του επί συζήτηση μία'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.tokens_to_string(tokens_src[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό το κείμενο ειναι ανεστραμμένο, όπως φαίνεται σε σύγκριση με το αρχικό κείμενο από το σύνολο δεδομένων:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Επιθυμείτε μία συζήτηση επί του θέματος τις επόμενες ημέρες, κατά τη διάρκεια της τρέχουσας περιόδου συνόδου.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Συγκρίνοντας αυτό το κείμενο με το αρχικό κείμενο από το σύνολο δεδομένων, παρατηρούμε οτι λειπουν εκτός από τα σημεία στίξης, και μερικές λέξεις(Επιθυμείτε). Αυτό οφείλεται στο γεγονός ότι χρησιμοποιούμε μόνο ένα λεξιλόγιο των 5000 πιο συχνών λέξεων απο το σύνολο δεδομένων. Οι λέξεις που επέλεξα ειναι αρκετά λίγες αλλά με το να έβαζα περισσότερες λέξεις η διαδικασια εκπαίδευσεις θα έπαιρνε ακόμα περισσότερο χρόνο.\n",
    "\n",
    "\n",
    "Παρακάτω βλέπουμε την ακολουθία των tokens για το αντίστοιχο κείμενο στη γλώσσα προορισμού. Βλέπουμε πώς είναι γεμισμένο με μηδενικά στο τέλος (post-padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   43,   20, 1864,    9,  123,   15,   14,  294,    7,    1,\n",
       "        194,    4,    1,  258,  323,  988,  301,   14,  159, 1067,    3,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dest[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss you have requested a debate on this subject in the course of the next few days during this part session eeee'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(tokens_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss You have requested a debate on this subject in the course of the next few days, during this part-session. eeee'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "\n",
    "Αφού πλέον έχουμε μετατρέψει τη βάση δεδομένων μας σε ακολουθίες από ακέραιους αριθμούς οι οποίοι είναι κατανεμημένη στην αρχή το τέλος της ακολουθίας μας είμαστε σε θέση να να ετοιμάσουμε τα δεδομένα για είσοδο στο νευρωνικό μας δίκτυο.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = tokens_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα δεδομένα εισόδου και εξόδου για τον decoder είναι πανομοιότυπα, εκτός από το γεγονός ότι τα δεδομένα εξόδου είναι κατά μία μονάδα ολισθημενα αριστερά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1235976, 55)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data = tokens_dest[:, :-1]\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1235976, 55)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data = tokens_dest[:, 1:]\n",
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,  418,   19,   43,   26,   20,  633,    1, 1427,    5,  182,\n",
       "          1,   81,    7,    9,  220,    4,   67, 2245,    9, 1638,    4,\n",
       "        849, 1821,    8, 1460,  108,    3,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 418,   19,   43,   26,   20,  633,    1, 1427,    5,  182,    1,\n",
       "         81,    7,    9,  220,    4,   67, 2245,    9, 1638,    4,  849,\n",
       "       1821,    8, 1460,  108,    3,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εάν χρησιμοποιήσουμε το tokenizer για να μετατρέψουμε αυτές τις ακολουθίες σε κείμενο, βλέπουμε ότι είναι ιδιες εκτός από την πρώτη λέξη που είναι 'ssss' που σηματοδοτεί την αρχή της πρότασης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss although as you will have seen the failed to still the people in a number of countries suffered a series of natural disasters that truly were eeee'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_input_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'although as you will have seen the failed to still the people in a number of countries suffered a series of natural disasters that truly were eeee'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_output_data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Neural Network\n",
    "\n",
    "### Create the Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτή είναι η είσοδος για τον encoder, ο οποίος λαμβάνει ακολουθιές ακέραιων συμβόλων. Το 'None' υποδεικνύει ότι οι ακολουθίες μπορούν να έχουν αυθαίρετο μήκος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None, ), name='encoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό είναι το μήκος για την έξοδο απο το embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό είναι embedding-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='encoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό είναι το μέγεθος των εσωτερικών καταστάσεων των (GRU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργούμε 3 GRU στρώματα που θα χαρτογραφούν από μια ακολουθία του embedding layer σε ένα και μοναδικό «thought vector» που συνοψίζει τα περιεχόμενα του κειμένου εισόδου. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
    "                   return_sequences=True)\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
    "                   return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
    "                   return_sequences=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper-function that connects all the layers of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoder():\n",
    "    # Start the neural network with its input-layer.\n",
    "    net = encoder_input\n",
    "    \n",
    "    # Connect the embedding-layer.\n",
    "    net = encoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "\n",
    "    # This is the output of the encoder.\n",
    "    encoder_output = net\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\christos\\anaconda2\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py:1456: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "encoder_output = connect_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Decoder\n",
    "\n",
    "\n",
    "Ο decoder λαμβάνει δύο εισόδους. Πρώτα χρειάζεται τον \"thought vector\" που παράγεται από τον encoder, ο οποίος συνοψίζει τα περιεχόμενα του κειμένου εισόδου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = Input(shape=(state_size,),\n",
    "                              name='decoder_initial_state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ο αποκωδικοποιητής χρειάζεται επίσης μια ακολουθία integer tokens ως είσοδο.  π.χ. που αντιστοιχεί στο κείμενο \"ssss once upon a time eeee\".\n",
    "\n",
    "Κατά τη διάρκεια  της μετάφρασης των νέων κειμένων εισόδου, θα αρχίσουμε με την τροφοδοσία μιας ακολουθίας με ένα token π.χ. το \"ssss\" το οποίο σηματοδοτεί την αρχή ενός κειμένου και σε συνδυασμό με τον \"thought vector\" από τον encoder, θα ελπίζουμε ότι θα μπορέσουμε να φτιάξουμε τη σωστή επόμενη λέξη π.χ. \"μια φορά\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None, ), name='decoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "έχουμε διαφορετικο embedding-layer για τον κωδικοποιητή και τον αποκωδικοποιητή επειδή έχουμε δύο διαφορετικά λεξιλόγια και δύο διαφορετικούς tokenizers για το γλώσσες προέλευσης και προορισμού."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='decoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργούμε 3 GRU στρώματα που θα χαρτογραφούν από μια ακολουθία του embedding layer σε ακολουθίες απο integer tokens που θα μπορούν να μετατραπούν στο  κειμένου εξόδου. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
    "                   return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
    "                   return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
    "                   return_sequences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα  GRU εξάγουν έναν tensor με το σχήμα `[batch_size, sequence_length, state_size]`, όπου κάθε λέξη κωδικοποιείται ως διάνυσμα μήκους `state_size`. Πρέπει να το μετατρέψουμε σε ακολουθίες insteger tokens που μπορούν να ερμηνευτούν ως λέξεις από το λεξιλόγιό μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words,\n",
    "                      activation='linear',\n",
    "                      name='decoder_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tο λειτουργικό ΑΡΙ του Keras,  επιτρέπει μεγαλύτερη ευελιξία στη σύνδεση των στρωμάτων π.χ. για τη δρομολόγηση διαφορετικών εισόδων στον decoder. Αυτό είναι χρήσιμο επειδή πρέπει να συνδέσουμε τον decoder απευθείας με τον encoder, αλλά θα συνδέσουμε και τον αποκωδικοποιητή σε άλλη είσοδο, ώστε να μπορέσουμε να τον εκτελέσουμε ξεχωριστά.\n",
    "\n",
    "function that connects all the layers of the decoder to some input of the initial-state values for the GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoder(initial_state):\n",
    "    # Start the decoder-network with its input-layer.\n",
    "    net = decoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    # Connect all the GRU-layers.\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "\n",
    "    # Connect the final dense layer that converts to\n",
    "    # one-hot encoded arrays.\n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect and Create the Models\n",
    "\n",
    "Αρχικά συνδέουμε τον encoder απευθείας στον decoder, ώστε να είναι ένα ολόκληρο μοντέλο που μπορεί να εκπαιδευτεί από άκρο σε άκρο. Αυτό σημαίνει ότι η αρχική κατάσταση του decoder ρυθμίζεται απο την έξοδο του encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input],\n",
    "                    outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια, δημιουργούμε ένα μοντέλο μόνο για τον encoder. Αυτό είναι χρήσιμο για τη χαρτογράφηση της αλληλουχίας απο integer tokens σε ένα «thought vector»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Στη συνέχεια, δημιουργούμε ένα μοντέλο μόνο για τον αποκωδικοποιητή . Αυτό μας επιτρέπει να εισάγουμε απευθείας την αρχική κατάσταση του decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
    "                      outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Πρέπει να σημειώσουμε ότι όλα αυτά τα μοντέλα χρησιμοποιούν τα ίδια βάρη και μεταβλητές. Απλώς αλλάζουμε τον τρόπο με τον οποίο συνδέονται. Έτσι, μόλις εκπαιδευτεί ολόκληρο το μοντέλο, μπορούμε να τρέξουμε ξεχωριστά τα μοντέλα κωδικοποιητή και αποκωδικοποιητή με τα εκπαιδευμένα βάρη."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "\n",
    "\n",
    "Η έξοδος του αποκωδικοποιητή είναι μια ακολουθία απο one-hot encoded arrays. Προκειμένου να εκπαιδεύσουμε τον αποκωδικοποιητή πρέπει να παρέχουμε τα επιθυμητά αποτελέσματα απο το training - set(one-hot encoded). Kαι στη συνέχεια να χρησιμοποιήσουμε μια λειτουργία απώλειας όπως η cross-entropy για να παράγουμε την επιθυμητή έξοδο.\n",
    "\n",
    "Ωστόσο, το σετ δεδομένων μας περιέχει integer tokens αντί για ένα one hot encoded arrays. Κάθε one-hot encoded array έχει 10000 στοιχεία, οπότε θα ήταν εξαιρετικά σπάταλη η μετατροπή ολόκληρου του συνόλου δεδομένων σε one - hot encoded array.\n",
    "\n",
    "\n",
    "O καλύτερος τρόπος είναι να χρησιμοποιήσετε μια λεγόμενη sparse_cross_entropy συνάρτηση, η οποία κάνει τη μετατροπή εσωτερικά από ακέραιους αριθμούς σε one hot encoded arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between y_true and y_pred.\n",
    "    \n",
    "    y_true is a 2-rank tensor with the desired output.\n",
    "    The shape is [batch_size, sequence_length] and it\n",
    "    contains sequences of integer-tokens.\n",
    "\n",
    "    y_pred is the decoder's output which is a 3-rank tensor\n",
    "    with shape [batch_size, sequence_length, num_words]\n",
    "    so that for each sequence in the batch there is a one-hot\n",
    "    encoded array of length num_words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss. This outputs a\n",
    "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire 2-rank tensor, we reduce it\n",
    "    # to a single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Training Model\n",
    "\n",
    "using the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deduce the correct shape of the decoder's output data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model using our custom loss-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\christos\\anaconda2\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model_train.compile(optimizer=optimizer,\n",
    "                    loss=sparse_cross_entropy,\n",
    "                    target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions\n",
    "\n",
    "\n",
    "Κατά τη διάρκεια της εκπαίδευσης θέλουμε να αποθηκεύουμε σημεία ελέγχου και να καταγράψουμε την πρόοδο στο TensorBoard.\n",
    "\n",
    "This is the callback for writing checkpoints during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = '21_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint\n",
    "\n",
    "load last checkpoint if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We wrap the data in named dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = \\\n",
    "{\n",
    "    'encoder_input': encoder_input_data,\n",
    "    'decoder_input': decoder_input_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = \\\n",
    "{\n",
    "    'decoder_output': decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θέλουμε ενα σύνολο απο 5000 ακολουθίες αλλα το Keras χρειάζεται αυτόν τον αριθμό ως κλάσμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004045385994550056"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_split = 5000 / len(encoder_input_data)\n",
    "validation_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Τώρα μπορούμε να εκπαιδεύσουμε το μοντέλο. Μία εποχή μου πήρε περίπου 6 ωρες για να ολοκληρωθεί. Το μοντέλο το εχω τρέξει 7 εποχές και το loss ειναι 1,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                batch_size=256,\n",
    "                epochs=3,\n",
    "                validation_split=validation_split,\n",
    "                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Texts\n",
    "\n",
    "This function translates a text from the source-language to the destination-language and optionally prints a true translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "    \"\"\"Translate a single text-string.\"\"\"\n",
    "\n",
    "    # Convert the input-text to integer-tokens.\n",
    "    # Note the sequence of tokens has to be reversed.\n",
    "    # Padding is probably not necessary.\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                reverse=True,\n",
    "                                                padding=True)\n",
    "    \n",
    "    # Get the output of the encoder's GRU which will be\n",
    "    # used as the initial state in the decoder's GRU.\n",
    "    # This could also have been the encoder's final state.\n",
    "\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "\n",
    "    # Max number of tokens / words in the output sequence.\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
    "    # This holds just a single sequence of integer-tokens,\n",
    "    # but the decoder-model expects a batch of sequences.\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    # While we haven't sampled the special end-token for ' eeee'\n",
    "    # and we haven't processed the max number of tokens.\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        # Update the input-sequence to the decoder\n",
    "        # with the last token that was sampled.\n",
    "        # In the first iteration this will set the\n",
    "        # first element to the start-token.\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        # Wrap the input-data in a dict for  safety,\n",
    "        x_data = \\\n",
    "        {\n",
    "            'decoder_initial_state': initial_state,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "\n",
    "       \n",
    "\n",
    "        # Input this data to the decoder and get the predicted output.\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        \n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens += 1\n",
    "\n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    \n",
    "    # Print the input-text.\n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "    # Print the translated output-text.\n",
    "    print(\"Translated text:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    # Optionally print the true translated text.\n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Από την πλευρά μας εξακολουθούμε να θεωρούμε ότι η οικονομική και κοινωνική συνοχή αποτελεί έναν από τους κεντρικούς στόχους της Ένωσης.\n",
      "\n",
      "Translated text:\n",
      " for our part we still have the economic and social cohesion of the union one of the objectives of the union eeee\n",
      "\n",
      "True output text:\n",
      "ssss We still feel that economic and social cohesion is one of the Union' s fundamental objectives. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 500\n",
    "translate(input_text=data_src[idx],\n",
    "          true_output_text=data_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      ". Ευχαριστώ, κύριε Επίτροπε Kinnock.\n",
      "\n",
      "Translated text:\n",
      " thank you commissioner eeee\n",
      "\n",
      "True output text:\n",
      "ssss Thank you, Commissioner Kinnock. eeee\n",
      "\n",
      "Tokenazed input text \n",
      "επίτροπε κύριε ευχαριστώ\n"
     ]
    }
   ],
   "source": [
    "idx = 856\n",
    "translate(input_text=data_src[idx],\n",
    "          true_output_text=data_dest[idx])\n",
    "print(\"Tokenazed input text \\n\" + tokenizer_src.tokens_to_string(tokens_src[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βλέπουμε ότι πραρολο που η πρόταση εισόδου περιέχει το όνομα Kinnock εμάς το νευρωνικό δίκτυο δεν περιέχει στο λεξιλόγιο του αυτήν την λέξη όποτε η μετάφραση θα γίνει βάση του λεξιλογίου του νευρωνικού δικτύου όπως βλέπουμε στην πρόταση Tokenazed input text ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Επιθυμείτε μία συζήτηση επί του θέματος τις επόμενες ημέρες, κατά τη διάρκεια της τρέχουσας περιόδου συνόδου.\n",
      "\n",
      "Translated text:\n",
      " we have a question on the next part session during the next part session eeee\n",
      "\n",
      "True output text:\n",
      "ssss You have requested a debate on this subject in the course of the next few days, during this part-session. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "translate(input_text=data_src[idx],\n",
    "          true_output_text=data_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Σας ευχαριστώ για την παρουσίαση.Είναι σημαντικό που η Επιτροπή δεν παραβλέπει ποτέ το γεγονός ότι η διαδικασία αναδιάρθρωσης θα είναι εντελώς χάσιμο χρόνου, εκτός εάν επανασυνδεθεί με τους ευρωπαίους πολίτες.\n",
      "\n",
      "Translated text:\n",
      " thank you very much the commission is convinced that the process of european integration will never be the end of the day the truth is that the citizens will be with the european project eeee\n",
      "\n",
      "True output text:\n",
      "ssss Thank you for the presentation. eeeessss It is important that the Commission never loses sight of the fact that the reform process will be a complete waste of time unless it reconnects with the European public. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 10100\n",
    "translate(input_text=data_src[idx] + data_src[idx+1],\n",
    "          true_output_text=data_dest[idx] + data_dest[idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Με θεωρείτε υπερβολικό!Κάθε ημέρα η επικαιρότητα φέρνει στο φως μια πρόσθετη απόδειξη της υποταγής μας.\n",
      "\n",
      "Translated text:\n",
      " in the day of day each day it is a positive sign of our ability to show our solidarity eeee\n",
      "\n",
      "True output text:\n",
      "ssss Do you think I am going over the top? eeeessss Every day the news brings more proof of our subservience. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 10200\n",
    "translate(input_text=data_src[idx] + data_src[idx+1],\n",
    "          true_output_text=data_dest[idx] + data_dest[idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "επίπεδο σε συνεργασία ευρωπαϊκή την επιθυμώ\n",
      "Input text:\n",
      "Επιθυμώ την ευρωπαϊκή συνεργασία σε διακυβερνητικό επίπεδο.Υποστηρίζω σθεναρά την ιδέα της διεθνούς συνεργασίας για την επίλυση κοινών προβλημάτων.\n",
      "\n",
      "Translated text:\n",
      " i would like to cooperation in the european common sense in the international context i support the idea of cooperation to promote the defence of international cooperation eeee\n",
      "\n",
      "True output text:\n",
      "ssss I want European cooperation at intergovernmental level. eeeessss I firmly support the idea of international cooperation to resolve common problems. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 375386\n",
    "print(tokenizer_src.tokens_to_string(tokens_src[idx]))\n",
    "translate(input_text=data_src[idx] + data_src[idx+1],\n",
    "          true_output_text=data_dest[idx] + data_dest[idx+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "πόλεμος\n",
      "\n",
      "Translated text:\n",
      " russia eeee\n",
      "\n",
      "True output text:\n",
      "war\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=\"πόλεμος\",\n",
    "          true_output_text='war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Τι ώρα είναι \n",
      "\n",
      "Translated text:\n",
      " what is the time for eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=\"Τι ώρα είναι \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Η Ελλάδα τολμά να ξαναβγεί στις αγορές\n",
      "\n",
      "Translated text:\n",
      " greece is being forced to adapt to the markets eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\n",
    "    input_text=\"Η Ελλάδα τολμά να ξαναβγεί στις αγορές\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Βρίσκομαι σε μία δύσκολη κατάσταση\n",
      "\n",
      "Translated text:\n",
      " i am in a difficult situation eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\n",
    "    input_text=\"Βρίσκομαι σε μία δύσκολη κατάσταση\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "ποιό είναι το μέλλον της ευρωπαικής ένωσης\n",
      "\n",
      "Translated text:\n",
      " it is the future of the union eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\n",
    "    input_text=\"ποιό είναι το μέλλον της ευρωπαικής ένωσης\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "θα γίνει πόλεμος \n",
      "\n",
      "Translated text:\n",
      " it will be a war eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\n",
    "    input_text=\"θα γίνει πόλεμος \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Συμπέρασμα\n",
    "\n",
    "Καθοριστικό σημείο για αυτή την άσκηση ήταν η επιλογή των αριθμών των λέξεων που θα χρησιμοποιεί το μοντέλο μας ή 5.000 λέξεις είναι πολύ λίγες και αυτό φαίνεται και από το αποτέλεσμα ότι το μοντέλο πολλές λέξεις τις αγνοεί εντελώς. αλλά όπως αναφέρθηκε και πιο πάνω το να επιλεχθούν περισσότερες λέξεις θα ήταν ακόμα πιο χρονοβόρο.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
